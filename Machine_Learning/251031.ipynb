{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b778362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 Module import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "\n",
    "# sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# 사실은 one-hot encopding처리는\n",
    "# pandas를 이용해서 더 많이 처리합니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 이상치 처리를 위해\n",
    "from scipy import stats\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afea18f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42\n"
     ]
    }
   ],
   "source": [
    "# Seed 설정\n",
    "def seed_everything(seed=42):\n",
    "\n",
    "    # 1. 환경변수\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # 2. Python, Numpy\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 3. TensorFlow\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # 4. PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)  # PyTorch 1.8+ 권장\n",
    "\n",
    "    print(f\"Seeds set to {seed}\")\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5245ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Raw Data Loading\n",
    "df = pd.read_csv('./Kaggle_project/Taitanic/data/train.csv')\n",
    "\n",
    "df\n",
    "# 필요없는 컬럼을 찾아서 삭제\n",
    "# 다행히 우리 데이터는 컬럼명이 존재하고\n",
    "# Data Dictionary가 있어요. 그래서 조금 더 쉽게 컬럼을 제어할 수\n",
    "# 있어요!\n",
    "# 상식에 기반해서 제거할 수 있어요 -> 확실한 방법은 아니예요!\n",
    "# 실제로는 상관계수같은걸 계산해서 수치적으로 필요없는 컬럼을 찾아서\n",
    "# 제거하는게 좋아요!\n",
    "# 필요없는 컬럼(독립변수) 4개로 설정\n",
    "# PassengerId, Name, Ticket, Cabin\n",
    "\n",
    "# 그런데..가만히 보니 이름안에 title이 포함되어 있어요!\n",
    "# 이런 데이터를 뽑아서 새로 Feature로 만들면 좋을 듯 해요!\n",
    "# 이렇게 새로 Feature를 추출해서 사용하는 방법도 많이 사용하는데\n",
    "# 지금 같은 경우는 Mr, Mrs, Miss, Master 를 추출해서 사용하면\n",
    "# Sex와 상당히 유사한 정보가 되요! (강하게 중복되고 거의 완전히\n",
    "# 상관성이 있다고 여길 수 있어요!)\n",
    "# => 다중공선성이 생기게 되요. feature redundancy(특성 중복)\n",
    "# => Multicollinearity\n",
    "# 다중공선성은\n",
    "# 1. 모델이 혼란스러워해요!\n",
    "# 2. 회귀계수가 안정되지 못해요! -> 일반화 성능이 떨어져요!\n",
    "# 그래서.. 우리는 Name에서 Title을 분리하지 않을꺼예요!\n",
    "\n",
    "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'],\n",
    "             axis=1,\n",
    "             inplace=False)\n",
    "df\n",
    "\n",
    "# 결측치와 각 컬럼의 값이 어떻게 구성되어 있는지 확인\n",
    "df.isnull().sum()\n",
    "# 결측치가 있는 컬럼은 Age(177), Embarked(2)\n",
    "df['Sex'].value_counts() # male, female 2개만 존재\n",
    "df['Embarked'].value_counts() # S, C, Q 3개만 존재\n",
    "\n",
    "# 데이터 타입부터 먼저 변경해줘요!\n",
    "# 성별부터 문자를 숫자로 변경\n",
    "gender_mapping = { 'male': 0, 'female': 1 }\n",
    "df['Sex'] = df['Sex'].map(gender_mapping)\n",
    "df\n",
    "\n",
    "# 가족수부터 처리해요!\n",
    "df['Family'] = df['SibSp'] + df['Parch']\n",
    "df = df.drop(['SibSp', 'Parch'], axis=1, inplace=False)\n",
    "df\n",
    "\n",
    "# Embarked는 결측치를 2개 가지고 있어요!\n",
    "# 빈도가 가장 많은 값을 찾아서 그 값으로 먼저 결측치 부터\n",
    "# 채워줄꺼에요!\n",
    "Embarked_freq = df['Embarked'].mode()[0]\n",
    "df['Embarked'] = df['Embarked'].fillna(Embarked_freq)\n",
    "\n",
    "# 문자를 숫자로 변경\n",
    "embarked_mapping = { 'S': 0, 'C' : 1, 'Q' : 2}\n",
    "df['Embarked'] = df['Embarked'].map(embarked_mapping)\n",
    "\n",
    "df = pd.get_dummies(df,\n",
    "                    columns=['Embarked'],\n",
    "                    drop_first=True)\n",
    "df\n",
    "\n",
    "# Age에 대해 결측치부터 처리 => 고민되요! => 평균으로 할께요\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "# Age에 대한 Bining처리\n",
    "# 알고리즘(라이브러리)을 이용한 처리가 권장되요!\n",
    "df['AgeBand'] = pd.cut(df['Age'],\n",
    "                       5,\n",
    "                       labels=[0,1,2,3,4])\n",
    "\n",
    "# Age에 대한 One-Hot처리\n",
    "df = pd.get_dummies(df,\n",
    "                    columns=['AgeBand'],\n",
    "                    drop_first=True)\n",
    "\n",
    "# Fare에 대해서도 Bining처리를 진행\n",
    "df['FareBand'] = pd.cut(df['Fare'],\n",
    "                       4,\n",
    "                       labels=[0,1,2,3])\n",
    "df = df.drop('Fare', axis=1, inplace=False)\n",
    "# One-Hot으로 변경\n",
    "df = pd.get_dummies(df,\n",
    "                    columns=['FareBand'],\n",
    "                    drop_first=True).astype(int)\n",
    "df.astype(int)\n",
    "\n",
    "df.shape # (891, 14)\n",
    "\n",
    "# 정규화는 필요없어요\n",
    "\n",
    "# 데이터 분할만 하면 될 듯 해요\n",
    "x_data = df.drop('Survived', axis=1).values\n",
    "y_data = df['Survived'].values.reshape(-1,1)\n",
    "\n",
    "x_data_train, x_data_test, y_data_train, y_data_test = \\\n",
    "train_test_split(x_data,\n",
    "                 y_data,\n",
    "                 test_size=0.2,\n",
    "                 stratify=y_data,\n",
    "                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e385261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7932960893854749 0.7086614173228346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kiyong/anaconda3/envs/data_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# sklearn Model 구현\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "sklearn_model = linear_model.LogisticRegression()\n",
    "\n",
    "# Model을 생성했으니 validation 실행\n",
    "# K-Fold Cross Validation\n",
    "score = cross_val_score(estimator=sklearn_model,\n",
    "                        X=x_data,\n",
    "                        y=y_data,\n",
    "                        scoring='accuracy',\n",
    "                        cv=5)\n",
    "\n",
    "score.mean() # 0.8035904839620865\n",
    "\n",
    "# 모델 학습\n",
    "sklearn_model.fit(x_data_train,\n",
    "                  y_data_train)\n",
    "\n",
    "# test데이터를 이용해서 예측값 도출\n",
    "sklearn_y_pred = sklearn_model.predict(x_data_test)\n",
    "\n",
    "# 예측값과 정답을 이용해서 실제 모델의 정확도를 측정\n",
    "sklearn_accuracy = accuracy_score(y_data_test, sklearn_y_pred)\n",
    "sklearn_f1 = f1_score(y_data_test, sklearn_y_pred)\n",
    "\n",
    "print(sklearn_accuracy, sklearn_f1)\n",
    "# sklearn_accuracy : 0.7932960893854749\n",
    "# sklearn_f1 : 0.7086614173228346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bb79778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6245 - loss: 1.6190 - val_accuracy: 0.6075 - val_loss: 1.6316\n",
      "Epoch 2/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6225 - loss: 1.5437 - val_accuracy: 0.6075 - val_loss: 1.5514\n",
      "Epoch 3/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6225 - loss: 1.4723 - val_accuracy: 0.6028 - val_loss: 1.4756\n",
      "Epoch 4/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6205 - loss: 1.4055 - val_accuracy: 0.5935 - val_loss: 1.4047\n",
      "Epoch 5/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6205 - loss: 1.3438 - val_accuracy: 0.5888 - val_loss: 1.3395\n",
      "Epoch 6/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6225 - loss: 1.2879 - val_accuracy: 0.5888 - val_loss: 1.2804\n",
      "Epoch 7/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6265 - loss: 1.2379 - val_accuracy: 0.6075 - val_loss: 1.2277\n",
      "Epoch 8/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6245 - loss: 1.1940 - val_accuracy: 0.5981 - val_loss: 1.1813\n",
      "Epoch 9/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6225 - loss: 1.1560 - val_accuracy: 0.5888 - val_loss: 1.1412\n",
      "Epoch 10/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6024 - loss: 1.1235 - val_accuracy: 0.5935 - val_loss: 1.1067\n",
      "Epoch 11/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5924 - loss: 1.0959 - val_accuracy: 0.5794 - val_loss: 1.0773\n",
      "Epoch 12/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5803 - loss: 1.0726 - val_accuracy: 0.5841 - val_loss: 1.0522\n",
      "Epoch 13/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5622 - loss: 1.0528 - val_accuracy: 0.5607 - val_loss: 1.0308\n",
      "Epoch 14/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5482 - loss: 1.0357 - val_accuracy: 0.5607 - val_loss: 1.0124\n",
      "Epoch 15/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5502 - loss: 1.0208 - val_accuracy: 0.5654 - val_loss: 0.9963\n",
      "Epoch 16/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5502 - loss: 1.0075 - val_accuracy: 0.5607 - val_loss: 0.9820\n",
      "Epoch 17/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5442 - loss: 0.9953 - val_accuracy: 0.5514 - val_loss: 0.9691\n",
      "Epoch 18/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5402 - loss: 0.9841 - val_accuracy: 0.5467 - val_loss: 0.9573\n",
      "Epoch 19/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5361 - loss: 0.9735 - val_accuracy: 0.5421 - val_loss: 0.9464\n",
      "Epoch 20/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5341 - loss: 0.9634 - val_accuracy: 0.5421 - val_loss: 0.9361\n",
      "Epoch 21/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5341 - loss: 0.9537 - val_accuracy: 0.5514 - val_loss: 0.9264\n",
      "Epoch 22/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5321 - loss: 0.9442 - val_accuracy: 0.5514 - val_loss: 0.9171\n",
      "Epoch 23/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5321 - loss: 0.9350 - val_accuracy: 0.5514 - val_loss: 0.9081\n",
      "Epoch 24/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5341 - loss: 0.9260 - val_accuracy: 0.5514 - val_loss: 0.8994\n",
      "Epoch 25/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5341 - loss: 0.9172 - val_accuracy: 0.5514 - val_loss: 0.8910\n",
      "Epoch 26/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5341 - loss: 0.9085 - val_accuracy: 0.5514 - val_loss: 0.8828\n",
      "Epoch 27/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5382 - loss: 0.9000 - val_accuracy: 0.5467 - val_loss: 0.8749\n",
      "Epoch 28/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5382 - loss: 0.8916 - val_accuracy: 0.5467 - val_loss: 0.8671\n",
      "Epoch 29/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5382 - loss: 0.8834 - val_accuracy: 0.5467 - val_loss: 0.8594\n",
      "Epoch 30/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5382 - loss: 0.8753 - val_accuracy: 0.5467 - val_loss: 0.8520\n",
      "Epoch 31/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5382 - loss: 0.8674 - val_accuracy: 0.5467 - val_loss: 0.8446\n",
      "Epoch 32/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5402 - loss: 0.8596 - val_accuracy: 0.5561 - val_loss: 0.8374\n",
      "Epoch 33/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5422 - loss: 0.8519 - val_accuracy: 0.5561 - val_loss: 0.8304\n",
      "Epoch 34/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5422 - loss: 0.8444 - val_accuracy: 0.5561 - val_loss: 0.8235\n",
      "Epoch 35/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5422 - loss: 0.8370 - val_accuracy: 0.5561 - val_loss: 0.8167\n",
      "Epoch 36/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5402 - loss: 0.8297 - val_accuracy: 0.5561 - val_loss: 0.8100\n",
      "Epoch 37/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5442 - loss: 0.8225 - val_accuracy: 0.5607 - val_loss: 0.8035\n",
      "Epoch 38/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5542 - loss: 0.8155 - val_accuracy: 0.5654 - val_loss: 0.7970\n",
      "Epoch 39/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5542 - loss: 0.8086 - val_accuracy: 0.5654 - val_loss: 0.7907\n",
      "Epoch 40/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5562 - loss: 0.8017 - val_accuracy: 0.5654 - val_loss: 0.7846\n",
      "Epoch 41/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5582 - loss: 0.7951 - val_accuracy: 0.5748 - val_loss: 0.7785\n",
      "Epoch 42/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5582 - loss: 0.7885 - val_accuracy: 0.5794 - val_loss: 0.7725\n",
      "Epoch 43/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5703 - loss: 0.7820 - val_accuracy: 0.5841 - val_loss: 0.7667\n",
      "Epoch 44/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5723 - loss: 0.7757 - val_accuracy: 0.5841 - val_loss: 0.7610\n",
      "Epoch 45/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5743 - loss: 0.7695 - val_accuracy: 0.5841 - val_loss: 0.7554\n",
      "Epoch 46/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5743 - loss: 0.7634 - val_accuracy: 0.5841 - val_loss: 0.7498\n",
      "Epoch 47/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5823 - loss: 0.7574 - val_accuracy: 0.6075 - val_loss: 0.7444\n",
      "Epoch 48/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5944 - loss: 0.7515 - val_accuracy: 0.6075 - val_loss: 0.7392\n",
      "Epoch 49/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5944 - loss: 0.7458 - val_accuracy: 0.6075 - val_loss: 0.7340\n",
      "Epoch 50/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6064 - loss: 0.7401 - val_accuracy: 0.6075 - val_loss: 0.7289\n",
      "Epoch 51/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6124 - loss: 0.7346 - val_accuracy: 0.6075 - val_loss: 0.7239\n",
      "Epoch 52/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6124 - loss: 0.7291 - val_accuracy: 0.6121 - val_loss: 0.7191\n",
      "Epoch 53/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6145 - loss: 0.7238 - val_accuracy: 0.6308 - val_loss: 0.7143\n",
      "Epoch 54/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6245 - loss: 0.7186 - val_accuracy: 0.6262 - val_loss: 0.7096\n",
      "Epoch 55/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6245 - loss: 0.7135 - val_accuracy: 0.6262 - val_loss: 0.7051\n",
      "Epoch 56/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6325 - loss: 0.7085 - val_accuracy: 0.6262 - val_loss: 0.7006\n",
      "Epoch 57/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6406 - loss: 0.7036 - val_accuracy: 0.6308 - val_loss: 0.6963\n",
      "Epoch 58/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6406 - loss: 0.6989 - val_accuracy: 0.6308 - val_loss: 0.6920\n",
      "Epoch 59/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6466 - loss: 0.6942 - val_accuracy: 0.6402 - val_loss: 0.6878\n",
      "Epoch 60/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6466 - loss: 0.6896 - val_accuracy: 0.6402 - val_loss: 0.6837\n",
      "Epoch 61/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6526 - loss: 0.6851 - val_accuracy: 0.6449 - val_loss: 0.6798\n",
      "Epoch 62/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6566 - loss: 0.6808 - val_accuracy: 0.6449 - val_loss: 0.6759\n",
      "Epoch 63/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6566 - loss: 0.6765 - val_accuracy: 0.6495 - val_loss: 0.6721\n",
      "Epoch 64/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6566 - loss: 0.6723 - val_accuracy: 0.6636 - val_loss: 0.6684\n",
      "Epoch 65/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6586 - loss: 0.6683 - val_accuracy: 0.6636 - val_loss: 0.6647\n",
      "Epoch 66/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6586 - loss: 0.6643 - val_accuracy: 0.6636 - val_loss: 0.6612\n",
      "Epoch 67/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6606 - loss: 0.6604 - val_accuracy: 0.6682 - val_loss: 0.6577\n",
      "Epoch 68/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6606 - loss: 0.6566 - val_accuracy: 0.6682 - val_loss: 0.6544\n",
      "Epoch 69/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6606 - loss: 0.6529 - val_accuracy: 0.6729 - val_loss: 0.6511\n",
      "Epoch 70/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6606 - loss: 0.6493 - val_accuracy: 0.6729 - val_loss: 0.6479\n",
      "Epoch 71/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6606 - loss: 0.6458 - val_accuracy: 0.6869 - val_loss: 0.6448\n",
      "Epoch 72/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6667 - loss: 0.6424 - val_accuracy: 0.6869 - val_loss: 0.6417\n",
      "Epoch 73/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6727 - loss: 0.6390 - val_accuracy: 0.6916 - val_loss: 0.6388\n",
      "Epoch 74/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6727 - loss: 0.6357 - val_accuracy: 0.7056 - val_loss: 0.6359\n",
      "Epoch 75/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6727 - loss: 0.6326 - val_accuracy: 0.7103 - val_loss: 0.6330\n",
      "Epoch 76/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6767 - loss: 0.6295 - val_accuracy: 0.7056 - val_loss: 0.6303\n",
      "Epoch 77/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6767 - loss: 0.6264 - val_accuracy: 0.7056 - val_loss: 0.6276\n",
      "Epoch 78/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6767 - loss: 0.6235 - val_accuracy: 0.7056 - val_loss: 0.6250\n",
      "Epoch 79/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6767 - loss: 0.6206 - val_accuracy: 0.7056 - val_loss: 0.6224\n",
      "Epoch 80/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6727 - loss: 0.6178 - val_accuracy: 0.7056 - val_loss: 0.6199\n",
      "Epoch 81/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6667 - loss: 0.6151 - val_accuracy: 0.6869 - val_loss: 0.6175\n",
      "Epoch 82/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6687 - loss: 0.6124 - val_accuracy: 0.6963 - val_loss: 0.6152\n",
      "Epoch 83/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6687 - loss: 0.6098 - val_accuracy: 0.6963 - val_loss: 0.6129\n",
      "Epoch 84/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6707 - loss: 0.6073 - val_accuracy: 0.6916 - val_loss: 0.6106\n",
      "Epoch 85/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6747 - loss: 0.6048 - val_accuracy: 0.6916 - val_loss: 0.6084\n",
      "Epoch 86/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6787 - loss: 0.6024 - val_accuracy: 0.6916 - val_loss: 0.6063\n",
      "Epoch 87/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6767 - loss: 0.6001 - val_accuracy: 0.6869 - val_loss: 0.6042\n",
      "Epoch 88/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6747 - loss: 0.5978 - val_accuracy: 0.6869 - val_loss: 0.6022\n",
      "Epoch 89/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6787 - loss: 0.5956 - val_accuracy: 0.6869 - val_loss: 0.6002\n",
      "Epoch 90/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6807 - loss: 0.5934 - val_accuracy: 0.6869 - val_loss: 0.5983\n",
      "Epoch 91/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6807 - loss: 0.5913 - val_accuracy: 0.6869 - val_loss: 0.5964\n",
      "Epoch 92/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6787 - loss: 0.5892 - val_accuracy: 0.6963 - val_loss: 0.5946\n",
      "Epoch 93/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6827 - loss: 0.5872 - val_accuracy: 0.6916 - val_loss: 0.5928\n",
      "Epoch 94/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6867 - loss: 0.5852 - val_accuracy: 0.7056 - val_loss: 0.5910\n",
      "Epoch 95/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6908 - loss: 0.5833 - val_accuracy: 0.7056 - val_loss: 0.5893\n",
      "Epoch 96/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6988 - loss: 0.5814 - val_accuracy: 0.7056 - val_loss: 0.5877\n",
      "Epoch 97/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7008 - loss: 0.5796 - val_accuracy: 0.7150 - val_loss: 0.5860\n",
      "Epoch 98/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7088 - loss: 0.5778 - val_accuracy: 0.7103 - val_loss: 0.5844\n",
      "Epoch 99/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7108 - loss: 0.5760 - val_accuracy: 0.7103 - val_loss: 0.5829\n",
      "Epoch 100/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7108 - loss: 0.5743 - val_accuracy: 0.7056 - val_loss: 0.5814\n",
      "Epoch 101/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7108 - loss: 0.5726 - val_accuracy: 0.7103 - val_loss: 0.5799\n",
      "Epoch 102/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7149 - loss: 0.5710 - val_accuracy: 0.7103 - val_loss: 0.5784\n",
      "Epoch 103/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7149 - loss: 0.5694 - val_accuracy: 0.7103 - val_loss: 0.5770\n",
      "Epoch 104/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7189 - loss: 0.5678 - val_accuracy: 0.7103 - val_loss: 0.5756\n",
      "Epoch 105/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7209 - loss: 0.5663 - val_accuracy: 0.7103 - val_loss: 0.5743\n",
      "Epoch 106/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7249 - loss: 0.5648 - val_accuracy: 0.7150 - val_loss: 0.5730\n",
      "Epoch 107/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7349 - loss: 0.5633 - val_accuracy: 0.7243 - val_loss: 0.5717\n",
      "Epoch 108/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7410 - loss: 0.5619 - val_accuracy: 0.7290 - val_loss: 0.5704\n",
      "Epoch 109/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7470 - loss: 0.5605 - val_accuracy: 0.7290 - val_loss: 0.5691\n",
      "Epoch 110/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7490 - loss: 0.5591 - val_accuracy: 0.7290 - val_loss: 0.5679\n",
      "Epoch 111/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7490 - loss: 0.5577 - val_accuracy: 0.7383 - val_loss: 0.5667\n",
      "Epoch 112/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7550 - loss: 0.5564 - val_accuracy: 0.7383 - val_loss: 0.5656\n",
      "Epoch 113/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7570 - loss: 0.5551 - val_accuracy: 0.7477 - val_loss: 0.5644\n",
      "Epoch 114/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7610 - loss: 0.5538 - val_accuracy: 0.7523 - val_loss: 0.5633\n",
      "Epoch 115/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7610 - loss: 0.5526 - val_accuracy: 0.7523 - val_loss: 0.5622\n",
      "Epoch 116/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7691 - loss: 0.5514 - val_accuracy: 0.7570 - val_loss: 0.5611\n",
      "Epoch 117/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7691 - loss: 0.5502 - val_accuracy: 0.7570 - val_loss: 0.5600\n",
      "Epoch 118/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7711 - loss: 0.5490 - val_accuracy: 0.7570 - val_loss: 0.5590\n",
      "Epoch 119/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7751 - loss: 0.5478 - val_accuracy: 0.7570 - val_loss: 0.5580\n",
      "Epoch 120/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7771 - loss: 0.5467 - val_accuracy: 0.7617 - val_loss: 0.5570\n",
      "Epoch 121/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7791 - loss: 0.5456 - val_accuracy: 0.7617 - val_loss: 0.5560\n",
      "Epoch 122/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7811 - loss: 0.5445 - val_accuracy: 0.7617 - val_loss: 0.5550\n",
      "Epoch 123/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7831 - loss: 0.5434 - val_accuracy: 0.7664 - val_loss: 0.5541\n",
      "Epoch 124/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7851 - loss: 0.5423 - val_accuracy: 0.7710 - val_loss: 0.5531\n",
      "Epoch 125/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7871 - loss: 0.5413 - val_accuracy: 0.7710 - val_loss: 0.5522\n",
      "Epoch 126/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7871 - loss: 0.5403 - val_accuracy: 0.7757 - val_loss: 0.5513\n",
      "Epoch 127/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7871 - loss: 0.5392 - val_accuracy: 0.7804 - val_loss: 0.5504\n",
      "Epoch 128/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7871 - loss: 0.5383 - val_accuracy: 0.7757 - val_loss: 0.5496\n",
      "Epoch 129/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7932 - loss: 0.5373 - val_accuracy: 0.7757 - val_loss: 0.5487\n",
      "Epoch 130/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7932 - loss: 0.5363 - val_accuracy: 0.7757 - val_loss: 0.5479\n",
      "Epoch 131/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7932 - loss: 0.5354 - val_accuracy: 0.7757 - val_loss: 0.5470\n",
      "Epoch 132/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7972 - loss: 0.5344 - val_accuracy: 0.7710 - val_loss: 0.5462\n",
      "Epoch 133/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7992 - loss: 0.5335 - val_accuracy: 0.7804 - val_loss: 0.5454\n",
      "Epoch 134/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8032 - loss: 0.5326 - val_accuracy: 0.7804 - val_loss: 0.5446\n",
      "Epoch 135/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5317 - val_accuracy: 0.7804 - val_loss: 0.5438\n",
      "Epoch 136/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5309 - val_accuracy: 0.7804 - val_loss: 0.5431\n",
      "Epoch 137/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5300 - val_accuracy: 0.7804 - val_loss: 0.5423\n",
      "Epoch 138/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8072 - loss: 0.5291 - val_accuracy: 0.7804 - val_loss: 0.5416\n",
      "Epoch 139/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8092 - loss: 0.5283 - val_accuracy: 0.7804 - val_loss: 0.5408\n",
      "Epoch 140/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8092 - loss: 0.5275 - val_accuracy: 0.7804 - val_loss: 0.5401\n",
      "Epoch 141/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8092 - loss: 0.5267 - val_accuracy: 0.7804 - val_loss: 0.5394\n",
      "Epoch 142/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8092 - loss: 0.5259 - val_accuracy: 0.7804 - val_loss: 0.5387\n",
      "Epoch 143/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.5251 - val_accuracy: 0.7804 - val_loss: 0.5380\n",
      "Epoch 144/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.5243 - val_accuracy: 0.7804 - val_loss: 0.5373\n",
      "Epoch 145/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8032 - loss: 0.5235 - val_accuracy: 0.7804 - val_loss: 0.5366\n",
      "Epoch 146/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.5228 - val_accuracy: 0.7804 - val_loss: 0.5359\n",
      "Epoch 147/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8072 - loss: 0.5220 - val_accuracy: 0.7804 - val_loss: 0.5353\n",
      "Epoch 148/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.5213 - val_accuracy: 0.7804 - val_loss: 0.5346\n",
      "Epoch 149/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.5205 - val_accuracy: 0.7804 - val_loss: 0.5340\n",
      "Epoch 150/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.5198 - val_accuracy: 0.7804 - val_loss: 0.5334\n",
      "Epoch 151/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5191 - val_accuracy: 0.7804 - val_loss: 0.5327\n",
      "Epoch 152/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5184 - val_accuracy: 0.7804 - val_loss: 0.5321\n",
      "Epoch 153/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5177 - val_accuracy: 0.7804 - val_loss: 0.5315\n",
      "Epoch 154/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5170 - val_accuracy: 0.7804 - val_loss: 0.5309\n",
      "Epoch 155/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5163 - val_accuracy: 0.7850 - val_loss: 0.5303\n",
      "Epoch 156/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5157 - val_accuracy: 0.7804 - val_loss: 0.5297\n",
      "Epoch 157/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8052 - loss: 0.5150 - val_accuracy: 0.7850 - val_loss: 0.5291\n",
      "Epoch 158/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5143 - val_accuracy: 0.7850 - val_loss: 0.5285\n",
      "Epoch 159/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5137 - val_accuracy: 0.7850 - val_loss: 0.5280\n",
      "Epoch 160/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5131 - val_accuracy: 0.7850 - val_loss: 0.5274\n",
      "Epoch 161/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5124 - val_accuracy: 0.7850 - val_loss: 0.5269\n",
      "Epoch 162/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5118 - val_accuracy: 0.7850 - val_loss: 0.5263\n",
      "Epoch 163/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8052 - loss: 0.5112 - val_accuracy: 0.7850 - val_loss: 0.5258\n",
      "Epoch 164/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5106 - val_accuracy: 0.7850 - val_loss: 0.5252\n",
      "Epoch 165/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8072 - loss: 0.5100 - val_accuracy: 0.7850 - val_loss: 0.5247\n",
      "Epoch 166/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.5094 - val_accuracy: 0.7850 - val_loss: 0.5242\n",
      "Epoch 167/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.5088 - val_accuracy: 0.7850 - val_loss: 0.5236\n",
      "Epoch 168/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.5082 - val_accuracy: 0.7850 - val_loss: 0.5231\n",
      "Epoch 169/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8072 - loss: 0.5076 - val_accuracy: 0.7850 - val_loss: 0.5226\n",
      "Epoch 170/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.5071 - val_accuracy: 0.7850 - val_loss: 0.5221\n",
      "Epoch 171/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8072 - loss: 0.5065 - val_accuracy: 0.7850 - val_loss: 0.5216\n",
      "Epoch 172/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.5059 - val_accuracy: 0.7850 - val_loss: 0.5211\n",
      "Epoch 173/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.5054 - val_accuracy: 0.7804 - val_loss: 0.5206\n",
      "Epoch 174/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.5048 - val_accuracy: 0.7804 - val_loss: 0.5202\n",
      "Epoch 175/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5043 - val_accuracy: 0.7804 - val_loss: 0.5197\n",
      "Epoch 176/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5038 - val_accuracy: 0.7804 - val_loss: 0.5192\n",
      "Epoch 177/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8052 - loss: 0.5032 - val_accuracy: 0.7804 - val_loss: 0.5187\n",
      "Epoch 178/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8052 - loss: 0.5027 - val_accuracy: 0.7804 - val_loss: 0.5183\n",
      "Epoch 179/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.5022 - val_accuracy: 0.7804 - val_loss: 0.5178\n",
      "Epoch 180/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.5017 - val_accuracy: 0.7804 - val_loss: 0.5174\n",
      "Epoch 181/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.5012 - val_accuracy: 0.7804 - val_loss: 0.5169\n",
      "Epoch 182/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8032 - loss: 0.5007 - val_accuracy: 0.7804 - val_loss: 0.5165\n",
      "Epoch 183/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8032 - loss: 0.5002 - val_accuracy: 0.7804 - val_loss: 0.5160\n",
      "Epoch 184/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8032 - loss: 0.4997 - val_accuracy: 0.7804 - val_loss: 0.5156\n",
      "Epoch 185/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.4992 - val_accuracy: 0.7804 - val_loss: 0.5152\n",
      "Epoch 186/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.4987 - val_accuracy: 0.7804 - val_loss: 0.5147\n",
      "Epoch 187/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8012 - loss: 0.4983 - val_accuracy: 0.7804 - val_loss: 0.5143\n",
      "Epoch 188/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8012 - loss: 0.4978 - val_accuracy: 0.7804 - val_loss: 0.5139\n",
      "Epoch 189/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.4973 - val_accuracy: 0.7804 - val_loss: 0.5135\n",
      "Epoch 190/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.4969 - val_accuracy: 0.7804 - val_loss: 0.5131\n",
      "Epoch 191/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.4964 - val_accuracy: 0.7804 - val_loss: 0.5126\n",
      "Epoch 192/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.4960 - val_accuracy: 0.7804 - val_loss: 0.5122\n",
      "Epoch 193/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.4955 - val_accuracy: 0.7804 - val_loss: 0.5118\n",
      "Epoch 194/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8032 - loss: 0.4951 - val_accuracy: 0.7804 - val_loss: 0.5114\n",
      "Epoch 195/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8012 - loss: 0.4946 - val_accuracy: 0.7804 - val_loss: 0.5111\n",
      "Epoch 196/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8012 - loss: 0.4942 - val_accuracy: 0.7804 - val_loss: 0.5107\n",
      "Epoch 197/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8012 - loss: 0.4938 - val_accuracy: 0.7804 - val_loss: 0.5103\n",
      "Epoch 198/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8012 - loss: 0.4933 - val_accuracy: 0.7804 - val_loss: 0.5099\n",
      "Epoch 199/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8012 - loss: 0.4929 - val_accuracy: 0.7804 - val_loss: 0.5095\n",
      "Epoch 200/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8012 - loss: 0.4925 - val_accuracy: 0.7804 - val_loss: 0.5091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpoch 10/200\\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.6024 - loss: 1.1235 - val_accuracy: 0.5935 - val_loss: 1.1067\\nEpoch 11/200\\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.5924 - loss: 1.0959 - val_accuracy: 0.5794 - val_loss: 1.0773\\nEpoch 12/200\\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.5803 - loss: 1.0726 - val_accuracy: 0.5841 - val_loss: 1.0522\\nEpoch 13/200\\n...\\nEpoch 199/200\\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.8012 - loss: 0.4929 - val_accuracy: 0.7804 - val_loss: 0.5095\\nEpoch 200/200\\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.8012 - loss: 0.4925 - val_accuracy: 0.7804 - val_loss: 0.5091\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensorflow 구현\n",
    "\n",
    "# 모델생성\n",
    "keras_model = Sequential()\n",
    "\n",
    "# 레이어 추가\n",
    "keras_model.add(Input(shape=(13,)))\n",
    "keras_model.add(Dense(units=1,\n",
    "                      activation='sigmoid'))\n",
    "\n",
    "# 데이터는 Train / Valid / Test 3개로 구분\n",
    "# 모델 compile\n",
    "keras_model.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# callback 처리\n",
    "log_dir = './logs/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tb_cb = TensorBoard(log_dir=log_dir,\n",
    "                    histogram_freq = 1)\n",
    "\n",
    "# 모델학습\n",
    "keras_model.fit(x_data_train,\n",
    "                y_data_train,\n",
    "                epochs=200,\n",
    "                callbacks=[tb_cb],\n",
    "                batch_size = 64,\n",
    "                validation_split=0.3,\n",
    "                verbose=1)\n",
    "\n",
    "'''\n",
    "Epoch 10/200\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.6024 - loss: 1.1235 - val_accuracy: 0.5935 - val_loss: 1.1067\n",
    "Epoch 11/200\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.5924 - loss: 1.0959 - val_accuracy: 0.5794 - val_loss: 1.0773\n",
    "Epoch 12/200\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.5803 - loss: 1.0726 - val_accuracy: 0.5841 - val_loss: 1.0522\n",
    "Epoch 13/200\n",
    "...\n",
    "Epoch 199/200\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.8012 - loss: 0.4929 - val_accuracy: 0.7804 - val_loss: 0.5095\n",
    "Epoch 200/200\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.8012 - loss: 0.4925 - val_accuracy: 0.7804 - val_loss: 0.5091\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b399625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "0.776536312849162 0.6491228070175439\n"
     ]
    }
   ],
   "source": [
    "# 예측작업\n",
    "keras_y_pred = keras_model.predict(x_data_test)\n",
    "keras_y_pred_class = (keras_y_pred >= 0.5).astype(int)\n",
    "\n",
    "keras_accuracy = accuracy_score(y_data_test, keras_y_pred_class)\n",
    "keras_f1 = f1_score(y_data_test, keras_y_pred_class)\n",
    "\n",
    "print(keras_accuracy, keras_f1) # 0.776536312849162 0.6491228070175439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b49253ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "\n",
    "# 유사하게 구현하면 되요!\n",
    "# 구현했다고 치고!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e633786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 3개의 모델을 다 만들었으면 가장 성능이 좋은 모델을\n",
    "# 이용해서 결과 파일을 만들어서 케글에 제출!\n",
    "\n",
    "# 모델은 sklearn모델을 이용하도록 해요!\n",
    "# 일단 test데이터와 제출파일을 DataFrame으로 불러와요!\n",
    "test = pd.read_csv('./Kaggle_project/Taitanic/data/test.csv')\n",
    "submission = pd.read_csv('./Kaggle_project/Taitanic/data/gender_submission.csv')\n",
    "\n",
    "# test 데이터로 우리 모델에 입력을 넣어서\n",
    "# 예측값을 얻을려면 당연히... 전처리 해야 해요!\n",
    "test\n",
    "\n",
    "test = test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'],\n",
    "             axis=1,\n",
    "             inplace=False)\n",
    "test\n",
    "\n",
    "# 결측치와 각 컬럼의 값이 어떻게 구성되어 있는지 확인\n",
    "# test.isnull().sum()\n",
    "\n",
    "# 데이터 타입부터 먼저 변경해줘요!\n",
    "# 성별부터 문자를 숫자로 변경\n",
    "gender_mapping = { 'male': 0, 'female': 1 }\n",
    "test['Sex'] = test['Sex'].map(gender_mapping)\n",
    "test\n",
    "\n",
    "# 가족수부터 처리해요!\n",
    "test['Family'] = test['SibSp'] + test['Parch']\n",
    "test = test.drop(['SibSp', 'Parch'], axis=1, inplace=False)\n",
    "test\n",
    "\n",
    "# 문자를 숫자로 변경\n",
    "embarked_mapping = { 'S': 0, 'C' : 1, 'Q' : 2}\n",
    "test['Embarked'] = test['Embarked'].map(embarked_mapping)\n",
    "\n",
    "test = pd.get_dummies(test,\n",
    "                    columns=['Embarked'],\n",
    "                    drop_first=True)\n",
    "test\n",
    "\n",
    "# Age에 대해 결측치부터 처리 => 고민되요! => 평균으로 할께요\n",
    "test['Age'] = test['Age'].fillna(test['Age'].mean())\n",
    "# Age에 대한 Bining처리\n",
    "# 알고리즘(라이브러리)을 이용한 처리가 권장되요!\n",
    "test['AgeBand'] = pd.cut(test['Age'],\n",
    "                       5,\n",
    "                       labels=[0,1,2,3,4])\n",
    "test = test.drop('Age', axis=1, inplace=False)\n",
    "test\n",
    "# Age에 대한 One-Hot처리\n",
    "test = pd.get_dummies(test,\n",
    "                    columns=['AgeBand'],\n",
    "                    drop_first=True)\n",
    "test\n",
    "\n",
    "# Fare에 대해서도 결측치 및 Bining처리를 진행\n",
    "test['Fare'] = test['Fare'].fillna(test['Fare'].mean())\n",
    "test['FareBand'] = pd.cut(test['Fare'],\n",
    "                       4,\n",
    "                       labels=[0,1,2,3])\n",
    "test = test.drop('Fare', axis=1, inplace=False)\n",
    "# One-Hot처리\n",
    "test = pd.get_dummies(test,\n",
    "                    columns=['FareBand'],\n",
    "                    drop_first=True)\n",
    "\n",
    "# Pclass에 대한 one-Hot 처리\n",
    "test = pd.get_dummies(test,\n",
    "                    columns=['Pclass'],\n",
    "                    drop_first=True).astype(int)\n",
    "\n",
    "test.shape # (418, 13)\n",
    "test\n",
    "\n",
    "predict = sklearn_model.predict(test.values)\n",
    "\n",
    "submission['Survived'] = predict\n",
    "\n",
    "submission.to_csv('./my_titanic.csv',\n",
    "                  index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
